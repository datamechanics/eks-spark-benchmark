registry := 'gcr.io/dm-docker'
cluster := 'demo-staging.datamechanics.co'
api-key := ''
data-folder := 'gs://dm-tpc-benchmark/tpc-ds/data'

build-and-push tag='3.0.0-0.1':
    sbt assembly
    docker build -t {{registry}}/spark-benchmark:{{tag}} .
    docker push {{registry}}/spark-benchmark:{{tag}}

generate-data scale partitions aggregate:
    #!/usr/bin/env bash
    if [ "{{aggregate}}" = "true" ]; then
        OUTPUT_FOLDER="{{scale}}-aggregated"
    else
        OUTPUT_FOLDER="{{scale}}-{{partitions}}"
    fi
    curl --location --request POST 'https://{{cluster}}/api/apps/' \
    --header 'Content-Type: application/json' \
    --header 'X-Api-Key: {{api-key}}' \
    --data-raw '{
        "jobName": "tpcds-datagen-'"$OUTPUT_FOLDER"'",
        "configOverrides": {
            "type": "Scala",
            "sparkVersion": "3.0.0",
            "image": "gcr.io/dm-docker/spark-benchmark:3.0.0-0.1",
            "imagePullPolicy": "Always",
            "mainApplicationFile": "local:///opt/spark/examples/jars/eks-spark-benchmark-assembly-1.0.jar",
            "mainClass": "com.amazonaws.eks.tpcds.DataGeneration",
            "arguments": [
                "{{data-folder}}/'"$OUTPUT_FOLDER"'",
                "/opt/tpcds-kit/tools",
                "parquet",
                "{{scale}}",
                "{{partitions}}",
                "true",
                "{{aggregate}}",
                "false"
            ],
            "executor": {
                "cores": 4,
                "coreRequest": "3500m",
                "memory": "23g"
            },
            "sparkConf": {
                "spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version": "2",
                "spark.speculation": "false",
                "spark.network.timeout": "2400",
                "spark.sql.shuffle.partitions": "2000",
                "spark.kubernetes.memoryOverheadFactor": "0.1"
            }
        }
    }'
