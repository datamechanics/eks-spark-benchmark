registry := 'gcr.io/dm-docker'
cluster := 'demo-staging.datamechanics.co'
api-key := ''
data-folder := 'gs://dm-tpc-benchmark/tpc-ds/data'
result-folder := 'gs://dm-tpc-benchmark/tpc-ds/result'
resource-folder := 'gs://dm-tpc-benchmark/tpc-ds/resource'

build-and-push tag='3.0.0-0.2':
    sbt assembly
    docker build -t {{registry}}/spark-benchmark:{{tag}} .
    docker push {{registry}}/spark-benchmark:{{tag}}

generate-data scale partitions aggregate:
    #!/usr/bin/env bash
    if [ "{{aggregate}}" = "true" ]; then
        OUTPUT_FOLDER="{{scale}}-aggregated"
    else
        OUTPUT_FOLDER="{{scale}}-{{partitions}}"
    fi
    curl --location --request POST 'https://{{cluster}}/api/apps/' \
    --header 'Content-Type: application/json' \
    --header 'X-Api-Key: {{api-key}}' \
    --data-raw '{
        "jobName": "tpcds-datagen-'"$OUTPUT_FOLDER"'",
        "configOverrides": {
            "type": "Scala",
            "sparkVersion": "3.0.0",
            "image": "gcr.io/dm-docker/spark-benchmark:3.0.0-0.2",
            "imagePullPolicy": "Always",
            "mainApplicationFile": "local:///opt/spark/examples/jars/eks-spark-benchmark-assembly-1.0.jar",
            "mainClass": "com.amazonaws.eks.tpcds.DataGeneration",
            "arguments": [
                "{{data-folder}}/'"$OUTPUT_FOLDER"'",
                "/opt/tpcds-kit/tools",
                "parquet",
                "{{scale}}",
                "{{partitions}}",
                "true",
                "{{aggregate}}",
                "false"
            ],
            "executor": {
                "cores": 4,
                "coreRequest": "3500m",
                "memory": "23g"
            },
            "sparkConf": {
                "spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version": "2",
                "spark.speculation": "false",
                "spark.network.timeout": "2400",
                "spark.sql.shuffle.partitions": "2000",
                "spark.kubernetes.memoryOverheadFactor": "0.1"
            }
        }
    }'

run-k8s-benchmark scale='10' iterations='1' optimize='false' queries='':
    #!/usr/bin/env bash
    INPUT_FOLDER="{{scale}}-aggregated"
    curl --location --request POST 'https://{{cluster}}/api/apps/' \
    --header 'Content-Type: application/json' \
    --header 'X-Api-Key: {{api-key}}' \
    --data-raw '{
        "jobName": "tpcds-queries-{{scale}}-{{optimize}}",
        "configOverrides": {
            "type": "Scala",
            "sparkVersion": "3.0.0",
            "image": "gcr.io/dm-docker/spark-benchmark:3.0.0-0.2",
            "imagePullPolicy": "Always",
            "mainApplicationFile": "local:///opt/spark/examples/jars/eks-spark-benchmark-assembly-1.0.jar",
            "mainClass": "com.amazonaws.eks.tpcds.BenchmarkSQL",
            "arguments": [
                "{{data-folder}}/'"$INPUT_FOLDER"'",
                "{{result-folder}}/'"$INPUT_FOLDER-$(date +'%s')"'",
                "/opt/tpcds-kit/tools",
                "parquet",
                "{{scale}}",
                "{{iterations}}",
                "{{optimize}}",
                "{{queries}}",
                "false"
            ],
            "executor": {
                "cores": 4,
                "coreRequest": "3500m",
                "memory": "23g",
                "instances": 5,
                "volumeMounts": [
                    {
                        "mountPath": "/tmp/spark-local-dir-1/",
                        "name": "spark-local-dir-1"
                    }
                ]
            },
            "driver": {
                "cores": 3,
                "memory": "10g",
                "nodeSelector": {
                    "beta.kubernetes.io/instance-type": "n1-standard-4"
                }
            },
            "volumes": [
                {
                    "name": "spark-local-dir-1",
                    "hostPath": {
                        "path": "/mnt/disks/ssd0/"
                    }
                }
            ],
            "sparkConf": {
                "spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version": "2",
                "spark.speculation": "false",
                "spark.network.timeout": "2400",
                "spark.sql.broadcastTimeout": "7200",
                "spark.sql.crossJoin.enabled": "true",
                "spark.sql.parquet.mergeSchema": "false",
                "spark.sql.parquet.filterPushdown": "true",
                "spark.dynamicAllocation.enabled": "false",
                "spark.scheduler.minRegisteredResourcesRatio": "1.0",
                "spark.scheduler.maxRegisteredResourcesWaitingTime": "3600s",
                "spark.local.dir": "/tmp/spark-local-dir-1/"
            }
        }
    }'

run-k8s-benchmark-no-ssd scale='10' iterations='1' optimize='false' queries='':
    #!/usr/bin/env bash
    INPUT_FOLDER="{{scale}}-aggregated"
    curl --location --request POST 'https://{{cluster}}/api/apps/' \
    --header 'Content-Type: application/json' \
    --header 'X-Api-Key: {{api-key}}' \
    --data-raw '{
        "jobName": "tpcds-queries-{{scale}}-{{optimize}}",
        "configOverrides": {
            "type": "Scala",
            "sparkVersion": "3.0.0",
            "image": "gcr.io/dm-docker/spark-benchmark:3.0.0-0.2",
            "imagePullPolicy": "Always",
            "mainApplicationFile": "local:///opt/spark/examples/jars/eks-spark-benchmark-assembly-1.0.jar",
            "mainClass": "com.amazonaws.eks.tpcds.BenchmarkSQL",
            "arguments": [
                "{{data-folder}}/'"$INPUT_FOLDER"'",
                "{{result-folder}}/'"$INPUT_FOLDER-$(date +'%s')"'",
                "/opt/tpcds-kit/tools",
                "parquet",
                "{{scale}}",
                "{{iterations}}",
                "{{optimize}}",
                "{{queries}}",
                "false"
            ],
            "executor": {
                "cores": 4,
                "coreRequest": "3500m",
                "memory": "23g",
                "instances": 5
            },
            "driver": {
                "cores": 3,
                "memory": "10g",
                "nodeSelector": {
                    "beta.kubernetes.io/instance-type": "n1-standard-4"
                }
            },
            "sparkConf": {
                "spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version": "2",
                "spark.speculation": "false",
                "spark.network.timeout": "2400",
                "spark.sql.broadcastTimeout": "7200",
                "spark.sql.crossJoin.enabled": "true",
                "spark.sql.parquet.mergeSchema": "false",
                "spark.sql.parquet.filterPushdown": "true",
                "spark.dynamicAllocation.enabled": "false",
                "spark.scheduler.minRegisteredResourcesRatio": "1.0",
                "spark.scheduler.maxRegisteredResourcesWaitingTime": "3600s"
            }
        }
    }'

upload-yarn-resources:
    gsutil cp 'target/scala-2.12/*.jar' '{{resource-folder}}/jars/'
    gsutil cp dataproc-init-script.sh '{{resource-folder}}/init-scripts/dataproc-init-script.sh'

run-yarn-benchmark scale='10' iterations='1' optimize='false' queries='':
    #!/usr/bin/env bash
    INPUT_FOLDER="{{scale}}-aggregated"
    WORKFLOW_TEMPLATE_NAME=tpcds-queries-{{scale}}-{{optimize}}
    LOCATION=europe-west1
    gcloud -q dataproc workflow-templates delete $WORKFLOW_TEMPLATE_NAME --region $LOCATION || true
    gcloud dataproc workflow-templates create $WORKFLOW_TEMPLATE_NAME --region=$LOCATION
    gcloud dataproc workflow-templates set-managed-cluster $WORKFLOW_TEMPLATE_NAME \
        --initialization-actions {{resource-folder}}/init-scripts/dataproc-init-script.sh \
        --region $LOCATION \
        --image-version preview \
        --master-machine-type n1-standard-4 \
        --worker-machine-type n2-highmem-4 \
        --num-workers 5 \
        --num-worker-local-ssds 1 \
        --cluster-name $WORKFLOW_TEMPLATE_NAME-cluster \
        --properties "\
    spark:spark.driver.cores=3,\
    spark:spark.driver.memory=10g,\
    spark:spark.executor.cores=4,\
    spark:spark.executor.memory=21g,\
    spark:spark.kubernetes.executor.request.cores=3500m,\
    spark:spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2,\
    spark:spark.speculation=false,\
    spark:spark.network.timeout=2400,\
    spark:spark.sql.broadcastTimeout=7200,\
    spark:spark.sql.crossJoin.enabled=true,\
    spark:spark.sql.parquet.mergeSchema=false,\
    spark:spark.sql.parquet.filterPushdown=true,\
    spark:spark.dynamicAllocation.enabled=false,\
    spark:spark.scheduler.minRegisteredResourcesRatio=1.0,\
    spark:spark.scheduler.maxRegisteredResourcesWaitingTime=3600s,\
    spark:spark.local.dir=/mnt/ssd0/"
    gcloud dataproc workflow-templates add-job spark \
        --jars {{resource-folder}}/jars/eks-spark-benchmark-assembly-1.0.jar \
        --class com.amazonaws.eks.tpcds.BenchmarkSQL \
        --step-id run-queries \
        --region $LOCATION \
        --workflow-template $WORKFLOW_TEMPLATE_NAME \
        -- \
        {{data-folder}}/$INPUT_FOLDER \
        {{result-folder}}/$INPUT_FOLDER-$(date +'%s') \
        /opt/tpcds-kit/tools \
        parquet \
        {{scale}} \
        {{iterations}} \
        {{optimize}} \
        "{{queries}}" \
        false
    gcloud dataproc workflow-templates instantiate $WORKFLOW_TEMPLATE_NAME --region $LOCATION
